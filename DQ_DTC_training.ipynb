{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LeakyReLU, ELU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from CustomKerasRL2Callbacks_torqueCtrl import StoreEpisodeLogger, randomSpeedProfile\n",
    "from gym.wrappers import FlattenObservation\n",
    "from gym.core import Wrapper\n",
    "from gym.spaces import Box, Tuple\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import gym_electric_motor as gem\n",
    "from gym_electric_motor.reward_functions import WeightedSumOfErrors\n",
    "from gym_electric_motor.physical_systems import ExternalSpeedLoad\n",
    "from gym_electric_motor.reference_generators import ConstReferenceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformObservationWrapper(Wrapper):\n",
    "    \"\"\"\n",
    "    This wrapper function receives the outputs from the GEM simulation (observation, reward, done flag)\n",
    "    and processes them as required. The original information from GEM is then overwritten.\n",
    "    \"\"\"\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        self.observation_space = Tuple((Box(\n",
    "            np.concatenate(([environment.observation_space[0].low[0]], # angular velocity\n",
    "                            environment.observation_space[0].low[5:7], # currents in dq\n",
    "                            environment.observation_space[0].low[10:12], # voltages in dq\n",
    "                            [-1, -1], # angles in cos, sin \n",
    "                            [-1])), # stator current\n",
    "            np.concatenate(([environment.observation_space[0].high[0]],\n",
    "                            environment.observation_space[0].high[5:7],\n",
    "                            environment.observation_space[0].high[10:12],\n",
    "                            [+1, +1],\n",
    "                            [+1])),\n",
    "        ), environment.observation_space[1])) # reference torque\n",
    "\n",
    "        self.subactions = -np.power(-1, self.env.physical_system._converter._subactions)\n",
    "        self.gamma = self.env.reward_function._gamma\n",
    "        self.test = False\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        (state, ref), rew, term, info = self.env.step(action)\n",
    "\n",
    "        self._obs_logger = np.concatenate((state, ref))\n",
    "\n",
    "        eps = state[12] * np.pi\n",
    "        angle_scale = 0.1\n",
    "        angles = [angle_scale * np.cos(eps), angle_scale * np.sin(eps)]\n",
    "\n",
    "        # transform the action information to the dq-frame\n",
    "        u_abc = self.subactions[action]\n",
    "        u_dq = self.env.physical_system.abc_to_dq_space(u_abc, epsilon_el=eps)\n",
    "        now_requested_voltage = u_dq\n",
    "\n",
    "        i_d = state[5]\n",
    "        i_q = state[6]\n",
    "        T = state[1]\n",
    "        T_ref = ref[0]\n",
    "\n",
    "        current_total = np.sqrt(i_d ** 2 + i_q ** 2) # calculate stator current i_s\n",
    "\n",
    "        # redefine the state observation vector o\n",
    "        # please note that all values are already normalized!\n",
    "        observable_state = np.concatenate(([state[0]],\n",
    "                                           state[5:7],\n",
    "                                           now_requested_voltage, \n",
    "                                           angles, \n",
    "                                           [2 * current_total - 1]))\n",
    "\n",
    "        # redefine the reward function\n",
    "        id_boundary = 15 / 270\n",
    "        dangerzone_boundary = 240 / 270\n",
    "        torque_boundary = 5 / self.env.limits[1]\n",
    "        e_T_abs = np.abs(T_ref - T)\n",
    "        term = False # termination flag\n",
    "        \n",
    "        if current_total > 1: # region E, \"Error zone\", set the terminal flag\n",
    "            rew = -1\n",
    "            term = True\n",
    "            \n",
    "        elif current_total > dangerzone_boundary: # region D, \"Danger zone\", short time overcurrent\n",
    "            reward_offset = - (1 - self.gamma)\n",
    "            rew = (1 - (current_total - dangerzone_boundary) / (1 - dangerzone_boundary)) * (1 - self.gamma) / 2 + reward_offset\n",
    "            \n",
    "        elif i_d > id_boundary: # region C, \"Caturation zone\", saturation of the permanent magnet\n",
    "            reward_offset = - (1 - self.gamma) / 2\n",
    "            rew = (1 - (i_d - id_boundary) / (dangerzone_boundary - id_boundary)) * (1 - self.gamma) / 2 + reward_offset\n",
    "            \n",
    "        elif e_T_abs > torque_boundary: # region B, \"Basic zone\", torque is not yet accurate\n",
    "            reward_offset = 0\n",
    "            rew = (1 - e_T_abs / 2) * (1 - self.gamma) / 2 + reward_offset\n",
    "        \n",
    "        else: # region A, \"Awesome zone\", torque is accurate and current needs to be minimized\n",
    "            reward_offset = (1 - self.gamma) / 2\n",
    "            rew = (1 - current_total) * (1 - self.gamma) / 2 + reward_offset\n",
    "\n",
    "        return (observable_state, ref), rew, term, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state, ref = self.env.reset()\n",
    "\n",
    "        self._obs_logger = np.concatenate((state, ref))\n",
    "\n",
    "        eps = state[12] * np.pi\n",
    "        angle_scale = 0.1\n",
    "        angles = [angle_scale * np.cos(eps), angle_scale * np.sin(eps)]\n",
    "        torque_error = [(ref[0] - state[1]) / 2]\n",
    "\n",
    "        # it is assumed that immediately after the reset no voltage is applied \n",
    "        u_abc = self.subactions[0]\n",
    "        u_dq = self.env.physical_system.abc_to_dq_space(u_abc, epsilon_el=eps)\n",
    "        now_requested_voltage = u_dq\n",
    "\n",
    "        i_d = state[5]\n",
    "        i_q = state[6]\n",
    "\n",
    "        current_total = np.sqrt(i_d ** 2 + i_q ** 2) # calculate stator current i_s\n",
    "        observable_state = np.concatenate(([state[0]], \n",
    "                                           state[5:7], \n",
    "                                           now_requested_voltage, \n",
    "                                           angles, \n",
    "                                           [2 * current_total - 1])) \n",
    "\n",
    "        return (observable_state, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment parameters\n",
    "torque_ref_generator = ConstReferenceGenerator(reference_state='torque', reference_value=np.random.uniform(-1, 1))\n",
    "\n",
    "motor_parameter = dict(p=3,            # [p] = 1, nb of pole pairs\n",
    "                       r_s=17.932e-3,  # [r_s] = Ohm, stator resistance\n",
    "                       l_d=0.37e-3,    # [l_d] = H, d-axis inductance\n",
    "                       l_q=1.2e-3,     # [l_q] = H, q-axis inductance\n",
    "                       psi_p=65.65e-3, # [psi_p] = Vs, magnetic flux of the permanent magnet\n",
    "                       )  # BRUSA\n",
    "\n",
    "u_sup = 350\n",
    "nominal_values=dict(omega=12000*2*np.pi/60,\n",
    "                    i=240,\n",
    "                    u=u_sup)\n",
    "\n",
    "limit_values=nominal_values.copy()\n",
    "limit_values[\"i\"] = 270\n",
    "limit_values[\"torque\"] = 200\n",
    "\n",
    "sampling_time = 50e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training routine\n",
    "def train_agent(param_dict):\n",
    "    \n",
    "    # unpack the parameters\n",
    "    subfolder_name = param_dict[\"subfolder_name\"]\n",
    "\n",
    "    gamma = param_dict[\"gamma\"]\n",
    "\n",
    "    alpha0 = param_dict[\"alpha0\"]\n",
    "    alpha1 = param_dict[\"alpha1\"]\n",
    "    lr_reduction_start = param_dict[\"lr_reduction_start\"]\n",
    "    lr_reduction_interval = param_dict[\"lr_reduction_interval\"]\n",
    "\n",
    "    epsilon0 = param_dict[\"epsilon0\"]\n",
    "    epsilon1 = param_dict[\"epsilon1\"]\n",
    "    nb_policy_annealing_steps = param_dict[\"nb_policy_annealing_steps\"]\n",
    "\n",
    "    layers = param_dict[\"layers\"]\n",
    "    neurons = param_dict[\"neurons\"]\n",
    "    target_update_parameter = param_dict[\"target_update_parameter\"]\n",
    "\n",
    "    batch_size = param_dict[\"batch_size\"]\n",
    "    memory_size = param_dict[\"memory_size\"]\n",
    "    \n",
    "    nb_episode_steps = param_dict[\"nb_episode_steps\"]\n",
    "    nb_training_steps = param_dict[\"nb_training_steps\"]\n",
    "    \n",
    "    activation_fcn = param_dict[\"activation_fcn\"]\n",
    "    activation_fcn_parameter = param_dict[\"activation_fcn_parameter\"]\n",
    "    \n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "    # create the subfolder if it does not exist yet\n",
    "    Path(subfolder_name).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    random_profile_generator = randomSpeedProfile(maxSpeed=nominal_values[\"omega\"], \n",
    "                                                  epsLength=nb_episode_steps)\n",
    "\n",
    "    # create the PMSM environment\n",
    "    env = gem.make(\"Finite-TC-PMSM-v0\",\n",
    "                   motor = dict(\n",
    "                       motor_parameter=motor_parameter,\n",
    "                       limit_values=limit_values,\n",
    "                       nominal_values=nominal_values,\n",
    "                   ),\n",
    "                   supply=dict(u_nominal=u_sup),\n",
    "                   load=ExternalSpeedLoad(random_profile_generator.randomProfile, \n",
    "                                          tau=sampling_time),\n",
    "                   tau=sampling_time,\n",
    "                   reward_function=WeightedSumOfErrors(reward_weights={'torque': 1},  # but the reward distribution will be overwritten\n",
    "                                                              gamma=gamma), # by means of the defined wrapper function\n",
    "                   reference_generator=torque_ref_generator,\n",
    "                   ode_solver='scipy.solve_ivp'\n",
    "                   )\n",
    "\n",
    "    (x, r) = env.reset()\n",
    "    limits = env.physical_system.limits\n",
    "\n",
    "    # wrap the environment to preprocess the observation as desired (to overwrite observation, reward, done flag)\n",
    "    env = FlattenObservation(TransformObservationWrapper(env))\n",
    "\n",
    "    # create the feedforward multilayer perceptron to be used as DQN\n",
    "    # select special procedure for parameterized activations\n",
    "    if activation_fcn == \"leaky_relu\" or activation_fcn == \"elu\":\n",
    "        dense_activation_fcn = 'linear'\n",
    "    else:\n",
    "        dense_activation_fcn = activation_fcn\n",
    "    \n",
    "    \n",
    "    nb_actions = env.action_space.n\n",
    "    window_length = 1\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(window_length,) + env.observation_space.shape))\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(neurons, activation=dense_activation_fcn))\n",
    "        if activation_fcn == 'leaky_relu':\n",
    "            model.add(LeakyReLU(alpha=activation_fcn_parameter))\n",
    "        elif activation_fcn == 'elu':\n",
    "            model.add(ELU(alpha=activation_fcn_parameter))\n",
    "    model.add(Dense(nb_actions,\n",
    "                    activation='linear'\n",
    "                    ))\n",
    "\n",
    "    # define the DQN agent\n",
    "    memory = SequentialMemory(limit=memory_size, window_length=window_length)\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(eps=epsilon0),\n",
    "                                  attr='eps',\n",
    "                                  value_max=epsilon0,\n",
    "                                  value_min=epsilon1,\n",
    "                                  value_test=0,\n",
    "                                  nb_steps=nb_policy_annealing_steps)\n",
    "    agent = DQNAgent(model=model,\n",
    "                     nb_actions=nb_actions,\n",
    "                     gamma=gamma,\n",
    "                     batch_size=batch_size,\n",
    "                     memory=memory,\n",
    "                     memory_interval=1,\n",
    "                     policy=policy,\n",
    "                     train_interval=1,\n",
    "                     target_model_update=target_update_parameter,\n",
    "                     enable_double_dqn=False)\n",
    "\n",
    "    # compile the agent\n",
    "    agent.compile(Adam(lr=alpha0), metrics=['mse'])\n",
    "\n",
    "    # define the logger to save the episode data \n",
    "    logger = StoreEpisodeLogger(folder_name=subfolder_name,\n",
    "                                file_name=\"training_episode\",\n",
    "                                tau=sampling_time, \n",
    "                                limits=limits, training=True,\n",
    "                                lr_max=alpha0, lr_min=alpha1,\n",
    "                                nb_steps_start=lr_reduction_start,\n",
    "                                nb_steps_reduction=lr_reduction_interval,\n",
    "                                speed_generator=random_profile_generator,\n",
    "                                create_eps_logs=True)\n",
    "    \n",
    "    # start training the agent (this will take a while, about 3 days on my local machine!)\n",
    "    history = agent.fit(env,\n",
    "                        nb_steps=nb_training_steps,\n",
    "                        action_repetition=1,\n",
    "                        verbose=0,\n",
    "                        visualize=False,\n",
    "                        nb_max_episode_steps=nb_episode_steps,\n",
    "                        log_interval=10000,\n",
    "                        callbacks=[logger])\n",
    "    \n",
    "    # save the network weights after training such that they can be reused\n",
    "    agent.save_weights(filepath=subfolder_name + \"/\" + \"weights.hdf5\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\"subfolder_name\": \"DQ_DTC_agent0\",\n",
    "    \n",
    "              \"gamma\": 0.868,\n",
    "\n",
    "              \"alpha0\": 2.887e-5,\n",
    "              \"alpha1\": 1.736e-5,\n",
    "              \"lr_reduction_start\":     460000,\n",
    "              \"lr_reduction_interval\": 2710000,\n",
    "\n",
    "              \"layers\": 10,\n",
    "              \"neurons\": 560,\n",
    "\n",
    "              \"epsilon0\": 2.119e-1,\n",
    "              \"epsilon1\": 1.774e-1,\n",
    "              \"nb_policy_annealing_steps\": 2210000,\n",
    "\n",
    "              \"memory_size\": 365000,\n",
    "              \"batch_size\": 32,\n",
    "              \"target_update_parameter\": 2.096e-1,\n",
    "\n",
    "              \"nb_training_steps\": 3000000,\n",
    "              \"nb_episode_steps\": 14900,\n",
    "              \n",
    "              \"activation_fcn\": \"leaky_relu\", # one of the following:\n",
    "                                              # 'softplus', 'leaky_relu', 'elu',\n",
    "                                              # 'selu', 'sigmoid', 'tanh'\n",
    "              \n",
    "              \"activation_fcn_parameter\": 0.3425\n",
    "              }\n",
    "\n",
    "train_agent(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot_TimeDomain_torqueCtrl import plot_episode\n",
    "\n",
    "# this function will save a pdf of the corresponding episode to the \"Plots\" folder\n",
    "# a \"Plots\" folder will be created if there is none\n",
    "plot_episode(training_folder = \"DQ_DTC_agent0\",\n",
    "             episode_number = 0,\n",
    "             episode_type = \"training_episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DQDTC_req]",
   "language": "python",
   "name": "conda-env-DQDTC_req-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
